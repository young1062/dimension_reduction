<h1> Welcome </h1>
This webpage contains content for an introductory course in Dimension Reduction first offered by Harvard University's <a href="https://statistics.fas.harvard.edu/">Statistics Department</a> during the Fall 2019 semester.   This class is intended for a single semester and is targeted at upper level undergraduate students.  It covers classical topics such as principal component analysis, nonnegative matrix factorization, and clustering with illustrative applications. The goal of this course is not to provide a complete summary of existing methods or software packages.  Rather, using mostly linear algebra, probability, and some coding in R, we  selected techniques are explored and their strengths/weaknesses are discussed.  This course focuses largely on a geometrical perspective of dimension reduction algorithms and the curious and often surprising nature of high-dimensional data.  

This site contains a complete list of the covered topics and corresponding <a href="notes"> lecture notes</a> and in some cases <a href="numerics"> numerical examples </a>.  All files can be found and cloned from the corresponding github repository for educational use.

<h2> Acknowledgements and References </h2>

The original development and perspective was driven largely by an extremely helpful conversation with <a href="https://www.paulbendich.com/">Paul Bendich </a> of Duke University's Mathematics Department.  His insight on his dimension reduction course provided the guiding principle of focusing on geometrical intuition.  Secondly, the texts 
<ul>
<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/"> The Elements of Statistical Learning </a> by Trevor Hastie, Robert Tibrishani, and Jerome Friedman </li>
<li> <a href="https://astro.temple.edu/~alan/MMST/"> Modern Multivariate Statistical Techniques </a> by Alan Julian Izenmann </li>
</ul>
were instrumental the construction of the lecture notes.



