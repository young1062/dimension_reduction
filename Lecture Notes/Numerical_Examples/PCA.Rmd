---
title: "Principal Component Analysis (PCA) Examples"
author: "Statistics 185"
date: "9/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('MASS')
library('plotly')
write_matex <- function(x) {
  begin <- "\\begin{bmatrix}"
  end <- "\\end{bmatrix}"
  X <-
    apply(x, 1, function(x) {
      paste(
        paste(x, collapse = "&"),
        "\\\\"
      )
    })
  writeLines(c(begin, X, end))
}
```



# {.tabset}

This document contains two examples of an application of PCA.  The first example uses synthetic data in $\mathbb{R}^3$ with the specific aim of building intuition on the geometric implications of PCA using visualizations of the data.  The last two example uses data from [MNIST](http://yann.lecun.com/exdb/mnist/) (LeCun et al.) containing pixelated images each viewed as a random vector.  In the MNIST example, we focus on data compression using the first *k* principal loadings and scores to approximate images.

## Multivariate Normal Data

We will draw $N = `r N<-1000; N`$ samples from a Multivariate Gaussian distribution with mean $\mu = (5,5,5)^T$ and covariance
```{r, echo = FALSE}
set.seed(1) # fix random number generator for reproducibility
mu = c(5,5,5); # set the mean
Q <- svd(matrix(c(1,2,3,4,5,6,7,8,9), ncol = 3))$u # generate an orthonormal matrix
Sigma <- Q %*% diag(c(25 , 9, 1)) %*% t(Q) # calculate Sigma = Q D Q' by eigendecomposition
# N <- 100; # number of samples to draw
data <- mvrnorm(N,mu,Sigma) # draw samples from normal, each sample saved in a row
```

\begin{aligned}
\Sigma = 
```{r, echo = FALSE, results = 'asis'} 
write_matex(round(Sigma,2))
```
\end{aligned}


We can view these data in a 3d scatterplot.
```{r, echo = FALSE}
plot_ly(data.frame( x = data[,1], y= data[,2], z = data[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-2, 12)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-2, 12)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-2, 12)))
         )

```
Notice the oblong shape of the cloud of points. Rotating this image, it is clear that the data varies more in certain directions than in others. PCA determines a new orthonormal basis, in this case of $\mathbb{R}^3$.  In this basis, the coordinates of our data are uncorrelated and ordered from greatest to least variability.

We begin by centering the data
```{r}
data <- scale(data, center = TRUE, scale = FALSE) # subtracts mean from each column
```
which appears the same as the previous figure except centered around the origin.
```{r, echo = FALSE}
plot_ly(data.frame( x = data[,1], y= data[,2], z = data[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-7, 7)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-7, 7)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-7, 7)))
         )

```

To find the principal component scores and loadings, let us calculate the eigenvalue decomposition of the sample covariance matrix

```{r}
Sigmahat <- (t(data) %*% data)/N # calculate the sample covariance matrix, recall the data has been centered 
Sigmahat.eigen <- eigen(Sigmahat) # calculate the eigen decomposition of Sigmahat
y <- data %*% Sigmahat.eigen$vectors # calculate the scores saved in rows
```
Note that the estimated covariance is

\begin{aligned}
\hat{\Sigma} &=  
```{r, echo = FALSE, results = 'asis'} 
write_matex(round(Sigmahat,2))
``` 
\\
& = 
```{r, echo = FALSE, results = 'asis'}
write_matex(round(Sigmahat.eigen$vectors,2))
```

```{r, echo = FALSE, results = 'asis'}
write_matex(diag(round(Sigmahat.eigen$values,2)))
```

```{r, echo = FALSE, results = 'asis'}
write_matex(t(round(Sigmahat.eigen$vectors,2)))
```
\end{aligned}
so the principal component loadings are the columns of the lower matrix on the left.  The variance of the principal components are the entries of the diagonal matrix above.


For each datum, we can remove its component in the direction of $\vec{w}_1$, and again view of the projection onto the orthogonal complement of $\vec{w}_1.$

```{r, echo = FALSE}
data2 <- data - y[,1] %*% t(Sigmahat.eigen$vectors[,1]) # remove first PC
plot_ly(data.frame( x = data2[,1], y= data2[,2], z = data2[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-7, 7)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-7, 7)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-7, 7)))
         )
```

We can  continue this process by also removing, from each vector, its component in the direction of $\vec{w}_2.$
```{r, echo = FALSE}
data3 <- data2 - y[,2] %*% t(Sigmahat.eigen$vectors[,2]) # remove first PC
plot_ly(data.frame( x = data3[,1], y= data3[,2], z = data3[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "x<sub>1</sub>", range = c(-7, 7)),
                       yaxis = list(title = "x<sub>2</sub>", range = c(-7, 7)),
                       zaxis = list(title = "x<sub>3</sub>", range = c(-7, 7)))
         )
```


Finally, we can plot the data in the basis defined by the loadings.  This is equivalent to a scatterplot of the scores.

```{r}
plot_ly(data.frame( x = y[,1], y= y[,2], z = y[,3]),
        x = ~x, y = ~y, z = ~z,
        marker = list(size = 5)) %>%
  add_markers() %>%
  layout(scene = list( xaxis = list(title = "y<sub>1</sub>"),
                       yaxis = list(title = "y<sub>2</sub>"),
                       zaxis = list(title = "y<sub>3</sub>")),
         title = "Principal Component Scores"
         )
```
In this basis, the data forms an ellipsoid with principal axis directed along the $y_1,\,y_2, \text{ and }y_3$ axes.  This is a result of the uncorrelated nature of principal component scores.


## MNIST Dataset
MNIST a database of images of handwritten digits.  Each image in MNIST is a 28 x 28 pixel images of a single handwritten digit.  For each pixel, we have a number between 0 (black) and 255 (white) indicating the color of the pixel. We begin by loading the file *digits.Rdata* which contains 42,000 digits and their lables from the MNIST database. These data are stored in the list *digits*.  We view the images of the first 36 digits in the dataset with their respective labels.

```{r, echo = FALSE}
# load file with digits
load(file = "~/Documents/Teaching/Dimension Reduction - Fall 2019/Datasets/digits.Rdata")
# visualize the digits
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')
for (idx in 1:36) { 
    im <- matrix(digits$pixels[idx, ], ncol = 28, nrow = 28)
    image(1:28, 1:28, im,
          xaxt='n', main=paste(digits$labels[idx]))
}
```


Each image can be thought of as a $28 \times 28$ matrix.  We convert this each image matrix into a $28^2 = 784$ dimensional vector by stacking its columns, $\vec{a}_1,\dots,\vec{a}_{28}$, each a vector in $\mathbb{R}^{28}$.

\[
\left[\vec{a}_1,\dots, \vec{a}_{28} \right] \longmapsto \left[\begin{matrix} \vec{a}_1 \\ \vdots \\ \vec{a}_{28} \end{matrix}\right].
\]
The image vectors are saved in the data matrix *digits\$pixels* with one image per row.


For this exposition on PCA, let us focus on the zero digits.  The images of the first 36 zeros are
```{r}
digits0 <- digits$pixels[digits$labels ==0, ] # keep on the zero digits
# visualize the digits
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')
for (idx in 1:36) { 
    im <- matrix(digits0[idx, ], ncol = 28, nrow = 28)
    image(1:28, 1:28, im,
          xaxt='n', yaxt='n')
}
```


We will use the function *princomp* for PCA focusing on the zero digits.  This functions dates a data matrix (with one subject per row) and computes,

   * the center of the data in *PCA.digits\$center* as a vector, 
    
   * the principal component scores with the scores of datum *i* in the *i*th row of *PCA.digits\$scores* , 
    
   * the standard deviations of the principal component scores in *PCA.digits\$sdev*, 
    
   * and the principal component loadings as columns in the matrix *PCA.digits\$loadings*. 
   
```{r}
PCA.digits <- princomp(digits0)
```

For an approximation of datum *i* using $k$ principal components, we have

\[\vec{x}_i^{(k)} = \bar{x} + \sum_{s=1}^k y_{i,s}\vec{w}_s \]

In code, we can calculate the approximations for each image using the first $k$ principal component scores/loadings as follow.
```{r}
k <- 5
xbar <- PCA.digits$center
W <- PCA.digits$loadings
Y <- PCA.digits$scores
digits.approx <- matrix(NA, nrow = PCA.digits$n.obs, ncol = 28^2)
for (i in 1:PCA.digits$n.obs){
  digits.approx[i,] <- xbar # add the center to each row
}
digits.approx <- digits.approx + Y[,1:k] %*% t(W)[1:k,]
```

We can generate the images of the approximates and compare with the originals. 
```{r}
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')
for (idx in 1:36) { 
    im <- matrix(digits.approx[idx, ], ncol = 28, nrow = 28)
    image(1:28, 1:28, im,
          xaxt='n', yaxt='n')
}
```


Here is the same image using the first 50 principal components.
```{r, echo = FALSE}
k <- 50;
for (i in 1:PCA.digits$n.obs){
  digits.approx[i,] <- xbar # add the center to each row
}
digits.approx <- digits.approx + Y[,1:k] %*% t(W)[1:k,]
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')
for (idx in 1:36) { 
    im <- matrix(digits.approx[idx, ], ncol = 28, nrow = 28)
    image(1:28, 1:28, im,
          xaxt='n', yaxt='n')
}
```


Clearly the images using 50 principal components are clear and closer to the original. It is natural to wonder, how many principal components is enough.  To better answer this question, consider the *scree* plot which shows the variance of each principal component plotted in descending order.
```{r}
plot(1:784, PCA.digits$sdev^2,
     type = 'l',
     xlab = "Principal Component, d",
     ylab = expression(paste("Variance, ",lambda[d])))
```

This is plot indicates that a large portion of the variability in the data is captured by very few component.  We can also view the cumulative variance as a function of the number of principal components.  The red dashed lines indicate that 90% of the variability in the image is explained by the first 61 principal components.  Importantly, since nearly 100% of the variability in the data is accounted for by the first 200 principal components, PCA is indicating that *our data lives near a lower dimensional affine subspace of* $\mathbb{R}^{784}$!
```{r}
plot(1:784, cumsum(PCA.digits$sdev^2)/sum(PCA.digits$sdev^2),
     type = 'l',
     xlab = "Principal Component, d",
     ylab = "Cumulative Variance")
abline(h = 0.9, v = 61, col="red", lwd=1, lty=2)

```
