--- 
title: 'Homework #7'
author: 
date: 
output: html_document
url_color: blue
---

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, message = FALSE)
set.seed(1) # for reproducibility of results
require(mclust)
require(cluster)
require(kernlab)
require(scatterplot3d)
load("Homework7.rData")
```

## Instructions {.tabset}

*Hierarchical clustering:* You may use the command *out <- hclust(DISS, method = "method")* where method may be "single", "average", or "complete" for the chosen linkage and *print(out)* displays the resulting dendrogram.  The variable *DISS* is the dissimilarity or distance matrix for the data of interest.  The command *cutree(out, k = K)* gives the associated *K* cluster solution.  

*$k$-medoids clustering:* Use the command *out <- pam(data, K, metric = "euclidean")* to find a *K* cluster solution using $k$-medoids. The cluster labels are given in *out\$clustering*.  

*$k$-means clustering:* Use the command *out <- kmeans(data, centers = K)* to find a *K* cluster solution using $k$-means.  The cluster labels are given in *out\$cluster*.

*Gaussian mixtures:* Use the command *out <- Mclust(data, modelNames = name, G = K)* to find a *K* cluster solution using a Gaussian mixture model with $K$ component and covariance structure given by name.  To compare multiple models, use the command *out <- mclustBIC(data, G = , modelNames = )*.  Calling *plot(out)* shows the BIC curves for all methods and number of mixture components. Calling *summary(out)* gives the three best-performing methods and number of mixture component.  

*Spectral clustering:* Use the command *out <- specc(data, centers = K)* to find a *K* cluster solution using spectral clustering.  The algorithm defaults to a radial basis function with adaptively determined width parameter given in *out\@kernelf\@kpar*.  The cluster labels are given in  *out\@.Data*.

## Problems

1.  Blum - Problem 7.1

2. Hastie - Problem 14.5 

3. Hastie - Problem 14.2 
    
4. Izenman - Problem 12.3 
    
5.  A plot of the data contained in the matrix *smile* is comprised of four pieces (two eyes, a nose, and a mouth) shown below.

    ```{r, fig.width = 4,fig.align = 'center'}
    plot(smile, xlab = '', ylab = '')
    ```

    a.  Plot the BIC curves vs. number of clusters for each of the default Gaussian mixture models in the MClust package.  Which method gives the best BIC?  Plot the data with points colored by cluster in the optimal solution.  Do the BIC estimates effectively estimate the correct number of components? Explain.
    
    b.  Plot the *smile* data coloring points by the cluster ID, assuming 4 clusters, given by the best model-based clustering by BIC.
    
    c.  Plot the eigenvalues of the graph Laplacian matrix for spectral clustering using a radial basis function with bandwidth $\sigma = 1/150$. Do the eigenvalues effectively estimate the number of components? Explain.
    
    d. Plot the *smile* data coloring points by cluster ID, assuming 4 clusters, given by spectral clustering.
    
    e.  Show the clustering results, assuming 4 clusters, using single-linkage, average-linkage, and complete-linkage.  
    
    f.  Compare the results of all of the clustering methods used in this problem.
    
    
6.  Repeat problem 6 using the data contained in the variable *cassini* comprised of three components shown below.

    ```{r, fig.width = 4, fig.align = 'center'}
    plot(cassini, xlab = '', ylab = '')
    ```

    a.  Plot the BIC curves vs. number of clusters for each of the default Gaussian mixture models in the MClust package.  Which method gives the best BIC?  Plot the data with points colored by cluster in the optimal solution.  Do the BIC estimates effectively estimate the correct number of components? Explain.
    
    b.  Plot the *smile* data coloring points by the cluster ID, assuming 3 clusters, given by the best model-based clustering by BIC.
    
    c.  Plot the eigenvalues of the graph Laplacian matrix for spectral clustering using a radial basis function with bandwidth $\sigma = 1/800$. Do the eigenvalues effectively estimate the number of components? Explain.
    
    d. Plot the *smile* data coloring points by cluster ID, assuming 3 clusters, given by spectral clustering.
    
    e.  Show the clustering results, assuming 3 clusters, using single-linkage, average-linkage, and complete-linkage.
    
    f. Compare the results of all of the clustering methods used in this problem.